---
title: 'Lab08: Kernel Machines'
author: "Mehmet GÃ¶nen"
date: "November 19, 2018"
output: html_document
runtime: shiny
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
```

## Parameters
```{r}
set.seed(421)
# mean parameters
class_means <- matrix(c(+2.0, +2.0,
                        -2.0, +2.0,
                        -2.0, -2.0,
                        +2.0, -2.0,
                        -4.0, -4.0,
                        +4.0, +4.0,
                        -4.0, +4.0,
                        +4.0, -4.0), 2, 8)
# covariance parameters
class_covariances <- array(c(+0.8, -0.6, -0.6, +0.8,
                             +0.8, +0.6, +0.6, +0.8,
                             +0.8, -0.6, -0.6, +0.8,
                             +0.8, +0.6, +0.6, +0.8,
                             +0.4, +0.0, +0.0, +0.4,
                             +0.4, +0.0, +0.0, +0.4,
                             +0.4, +0.0, +0.0, +0.4,
                             +0.4, +0.0, +0.0, +0.4), c(2, 2, 8))
# sample sizes
class_sizes <- c(100, 100)
```

## Data Generation
```{r}
# generate random samples
points1 <- mvrnorm(n = class_sizes[1] / 4, mu = class_means[,1], Sigma = class_covariances[,,1])
points2 <- mvrnorm(n = class_sizes[2] / 4, mu = class_means[,2], Sigma = class_covariances[,,2])
points3 <- mvrnorm(n = class_sizes[1] / 4, mu = class_means[,3], Sigma = class_covariances[,,3])
points4 <- mvrnorm(n = class_sizes[2] / 4, mu = class_means[,4], Sigma = class_covariances[,,4])
points5 <- mvrnorm(n = class_sizes[2] / 4, mu = class_means[,5], Sigma = class_covariances[,,5])
points6 <- mvrnorm(n = class_sizes[2] / 4, mu = class_means[,6], Sigma = class_covariances[,,6])
points7 <- mvrnorm(n = class_sizes[1] / 4, mu = class_means[,7], Sigma = class_covariances[,,7])
points8 <- mvrnorm(n = class_sizes[1] / 4, mu = class_means[,8], Sigma = class_covariances[,,8])
X <- rbind(points1, points3, points7, points8, points2, points4, points5, points6)
colnames(X) <- c("x1", "x2")

# generate corresponding labels
y <- c(rep(1, class_sizes[1]), rep(0, class_sizes[2]))
```

## Exporting Data
```{r}
# write data to a file
write.csv(x = cbind(X, y), file = "lab08_data_set.csv", row.names = FALSE)
```

## Plotting Data
```{r, fig.height = 6, fig.width = 6}
# plot data points generated
plot(points1[,1], points1[,2], type = "p", pch = 19, col = "red", las = 1,
     xlim = c(-6, 6), ylim = c(-6, 6),
     xlab = "x1", ylab = "x2")
points(points2[,1], points2[,2], type = "p", pch = 19, col = "blue")
points(points3[,1], points3[,2], type = "p", pch = 19, col = "red")
points(points4[,1], points4[,2], type = "p", pch = 19, col = "blue")
points(points5[,1], points5[,2], type = "p", pch = 19, col = "blue")
points(points6[,1], points6[,2], type = "p", pch = 19, col = "blue")
points(points7[,1], points7[,2], type = "p", pch = 19, col = "red")
points(points8[,1], points8[,2], type = "p", pch = 19, col = "red")
```

## Importing Data
```{r}
# read data into memory
data_set <- read.csv("lab08_data_set.csv")

# get X and y values
X_train <- cbind(data_set$x1, data_set$x2)
y_train <- 2 * (data_set$y == 1) - 1

# get number of samples and number of features
N_train <- length(y_train)
D_train <- ncol(X_train)
```

## Distance and Kernel Functions
$$d(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = ||\boldsymbol{x}_{i} - \boldsymbol{x}_{j}||_{2} = \sqrt{(\boldsymbol{x}_{i} - \boldsymbol{x}_{j})^{\top} (\boldsymbol{x}_{i} - \boldsymbol{x}_{j})} = \sqrt{\sum\limits_{d = 1}^{D}(x_{id} - x_{jd})^{2}}$$
```{r}
# define Euclidean distance function
pdist <- function(X1, X2) {
  if (identical(X1, X2) == TRUE) {
    D <- as.matrix(dist(X1))
  }
  else {
    D <- as.matrix(dist(rbind(X1, X2)))
    D <- D[1:nrow(X1), (nrow(X1) + 1):(nrow(X1) + nrow(X2))]
  }
  return(D)
}
```
$$k(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \exp\left(-\dfrac{||\boldsymbol{x}_{i} - \boldsymbol{x}_{j}||_{2}^{2}}{2s^{2}}\right)$$
```{r}
# define Gaussian kernel function
gaussian_kernel <- function(X1, X2, s) {
  D <- pdist(X1, X2)
  K <- exp(-D^2 / (2 * s^2))
}
```

## Learning Algorithm
Primal Problem
$$
\begin{equation}
	\begin{split}
		\mbox{minimize}\;\;& \dfrac{1}{2} ||\boldsymbol{w}||_{2}^{2} + C \sum\limits_{i = 1}^{N} \xi_{i} \\
		\mbox{with respect to}\;\;& \boldsymbol{w} \in \mathbb{R}^{D},\;\; \boldsymbol{\xi} \in \mathbb{R}^{N},\;\; b \in \mathbb{R} \\
		\mbox{subject to}\;\;& y_{i} (\boldsymbol{w}^{\top} \boldsymbol{x}_{i} + b) \geq 1 - \xi_{i} \;\;\;\; i = 1,2,\dots,N \\
		& \xi_{i} \geq 0\;\;\;\; i = 1,2,\dots,N \\
		\mbox{where}\;\;& C \in \mathbb{R}_{+}
	\end{split}
\end{equation}$$

Dual Problem
$$
\begin{equation}
	\begin{split}
		\mbox{maximize}\;\;& \sum\limits_{i = 1}^{N} \alpha_{i} - \dfrac{1}{2} \sum\limits_{i = 1}^{N} \sum\limits_{j = 1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} k(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \\
		\mbox{with respect to}\;\;& \boldsymbol{\alpha} \in \mathbb{R}^{N} \\
		\mbox{subject to}\;\;& \sum\limits_{i = 1}^{N} \alpha_{i} y_{i} = 0 \\
		& 0 \leq \alpha_{i} \leq C\;\;\;\; i = 1,2,\dots,N \\
		\mbox{where}\;\;& C \in \mathbb{R}_{+}
	\end{split}
\end{equation}
$$

Dual Problem in Matrix-Vector Form
$$
\begin{equation}
	\begin{split}
		\mbox{minimize}\;\;&-\boldsymbol{1}^{\top} \boldsymbol{\alpha} + \dfrac{1}{2} \boldsymbol{\alpha}^{\top} ((y y^{\top}) \odot \mathbf{K}) \boldsymbol{\alpha} \\
		\mbox{with respect to}\;\; & \boldsymbol{\alpha} \in \mathbb{R}^{N} \\
		\mbox{subject to}\;\;& \boldsymbol{y}^{\top} \boldsymbol{\alpha} = 0 \\
		& \boldsymbol{0} \leq \boldsymbol{\alpha} \leq C \boldsymbol{1} \\
		\mbox{where}\;\;& C \in \mathbb{R}_{+}
	\end{split}
\end{equation}
$$

```{r}
inputPanel(
  sliderInput("s", label = "log2(Kernel width parameter) log2(s):", min = -2, max = 2, step = 1, value = 0, animate = TRUE),
  sliderInput("C", label = "log10(Regularization parameter) log10(C):", min = -2, max = 2, step = 1, value = 1, animate = TRUE)
)

renderPlot({
  # calculate Gaussian kernel
  s <- 2^as.numeric(input$s)
  K_train <- gaussian_kernel(X_train, X_train, s)
  yyK <- (y_train %*% t(y_train)) * K_train

  # set learning parameters
  C <- 10^as.numeric(input$C)
  epsilon <- 1e-3
  
  # add library required to solve QP problems
  library(kernlab)
  result <- ipop(c = rep(-1, N_train), H = yyK,
                 A = y_train, b = 0, r = 0,
                 l = rep(0, N_train), u = rep(C, N_train))
  alpha <- result@primal
  alpha[alpha < C * epsilon] <- 0
  alpha[alpha > C * (1 - epsilon)] <- C
  
  # find bias parameter
  support_indices <- which(alpha != 0)
  active_indices <- which(alpha != 0 & alpha < C)
  b <- mean(y_train[active_indices] * (1 - yyK[active_indices, support_indices] %*% alpha[support_indices]))
  
  # calculate predictions on training samples
  f_predicted <- K_train %*% (y_train * alpha) + b
  
  # calculate confusion matrix
  y_predicted <- 2 * (f_predicted > 0) - 1
  confusion_matrix <- table(y_predicted, y_train)
  print(confusion_matrix)
  
  # evaluate discriminant function on a grid
  x1_interval <- seq(from = -6, to = +6, by = 0.3)
  x2_interval <- seq(from = -6, to = +6, by = 0.3)
  x1_grid <- matrix(x1_interval, nrow = length(x1_interval), ncol = length(x1_interval), byrow = FALSE)
  x2_grid <- matrix(x2_interval, nrow = length(x2_interval), ncol = length(x2_interval), byrow = TRUE)
  
  K_test <- gaussian_kernel(cbind(as.numeric(x1_grid), as.numeric(x2_grid)), X_train, s)
  discriminant_values <- matrix(K_test %*% (y_train * alpha) + b, length(x1_interval), length(x2_interval))
  
  plot(X_train[y_train == 1, 1], X_train[y_train == 1, 2], 
       type = "p", pch = 19, col = "red",
       xlim = c(-6, +6), ylim = c(-6, +6),
       xlab = "x1", ylab = "x2", las = 1)
  points(X_train[y_train == -1, 1], X_train[y_train == -1, 2], type = "p", pch = 19, col = "blue")
  points(X_train[support_indices, 1], X_train[support_indices, 2], cex = 1.5, lwd = 2)
  points(x1_grid[discriminant_values > 0], x2_grid[discriminant_values > 0], col = rgb(red = 1, green = 0, blue = 0, alpha = 0.01), pch = 19, cex = 5)
  points(x1_grid[discriminant_values < 0], x2_grid[discriminant_values < 0], col = rgb(red = 0, green = 0, blue = 1, alpha = 0.01), pch = 19, cex = 5)
  contour(x1_interval, x2_interval, discriminant_values, levels = c(0, -1, +1), add = TRUE, lwd = 2, drawlabels = FALSE, lty = c(1, 2, 2))
}, width = 600, height = 600)
```