---
title: 'Lab01: Parametric Methods'
author: "Mehmet GÃ¶nen"
date: "September 23, 2019"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Parameters
```{r}
set.seed(421)
# mean parameters
class_means <- c(-3, -1, 3)
# standard deviation parameters
class_deviations <- c(1.2, 1.0, 1.3)
# sample sizes
class_sizes <- c(40, 30, 50)
```

## Data Generation
```{r}
# generate random samples
points1 <- rnorm(n = class_sizes[1], mean = class_means[1], sd = class_deviations[1])
points2 <- rnorm(n = class_sizes[2], mean = class_means[2], sd = class_deviations[2])
points3 <- rnorm(n = class_sizes[3], mean = class_means[3], sd = class_deviations[3])
x <- c(points1, points2, points3)

# generate corresponding labels
y <- c(rep(1, class_sizes[1]), rep(2, class_sizes[2]), rep(3, class_sizes[3]))
```

## Exporting Data
```{r}
# write data to a file
write.csv(x = cbind(x, y), file = "lab01_data_set.csv", row.names = FALSE)
```

## Plotting Data
```{r, fig.keep = "all"}
data_interval <- seq(from = -7, to = +7, by = 0.01)
density1 <- dnorm(data_interval, mean = class_means[1], sd = class_deviations[1])
density2 <- dnorm(data_interval, mean = class_means[2], sd = class_deviations[2])
density3 <- dnorm(data_interval, mean = class_means[3], sd = class_deviations[3])
# plot data points of the first class
plot(points1, rep(-0.01, class_sizes[1]), type = "p", pch = 19, col = "red",
     xlim = c(-7, +7), ylim = c(-0.03, max(density1, density2, density3)),
     xlab = "x", ylab = "density", las = 1)
# plot densify of the first class
points(data_interval, density1, type = "l", col = "red", lwd = 2)
# plot data points of the second class
points(points2, rep(-0.02, class_sizes[2]), type = "p", pch = 19, col = "green")
# plot densify of the second class
points(data_interval, density2, type = "l", col = "green", lwd = 2)
# plot data points of the third class
points(points3, rep(-0.03, class_sizes[3]), type = "p", pch = 19, col = "blue")
# plot densify of the third class
points(data_interval, density3, type = "l", col = "blue", lwd = 2)
```

## Importing Data
```{r}
# read data into memory
data_set <- read.csv("lab01_data_set.csv")

# get x and y values
x <- data_set$x
y <- data_set$y

# get number of classes and number of samples
K <- max(y)
N <- length(y)
```

## Parameter Estimation
$\mu_{c}^{\star} = \dfrac{\sum\limits_{i = 1}^{N} x_{i} \mathbf{1}(y_{i} = c)}{\sum\limits_{i = 1}^{N} \mathbf{1}(y_{i} = c)}$

```{r}
# calculate sample means
sample_means <- sapply(X = 1:K, FUN = function(c) {mean(x[y == c])})
```

${\sigma_{c}^{2}}^{\star} = \dfrac{\sum\limits_{i = 1}^{N} (x_{i} - \mu_{c}^{\star})^{2} \mathbf{1}(y_{i} = c)}{\sum\limits_{i = 1}^{N} \mathbf{1}(y_{i} = c)}$

```{r}
# calculate sample deviations
sample_deviations <- sapply(X = 1:K, FUN = function(c) {sqrt(mean((x[y == c] - sample_means[c])^2))})
```

$\hat{P}(y_{i} = c) = \dfrac{\sum\limits_{i = 1}^{N} \mathbf{1}(y_{i} = c)}{N}$

```{r}
# calculate prior probabilities
class_priors <- sapply(X = 1:K, FUN = function(c) {mean(y == c)})
```

## Parametric Classification
```{r}
data_interval <- seq(from = -7, to = +7, by = 0.01)
```

\begin{align*}
g_{c}(x) &= \log p(x | y = c) + \log P(y = c)\\
&= -\dfrac{1}{2} \log(2 \pi \sigma_{c}^{2}) - \dfrac{(x - \mu_{c})^{2}}{2 \sigma_{c}^{2}} + \log P(y = c)
\end{align*}

```{r}
# evaluate score functions
score_values <- sapply(X = 1:K, FUN = function(c) {- 0.5 * log(2 * pi * sample_deviations[c]^2) - 0.5 * (data_interval - sample_means[c])^2 / sample_deviations[c]^2 + log(class_priors[c])})
```

\begin{align*}
\log P(y = c | x) &= \log p(x | y = c) + \log P(y = c) - \log p(x)\\
\log p(x) &= \log\left(\sum \limits_{c = 1}^{K} p(x | y = c) P(y = c)\right)
\end{align*}

We used the following property to calculate $\log p(x)$:

$\log\left(\sum\limits_{i = 1}^{N} \exp(x_{i})\right) = m + \log\left(\sum\limits_{i = 1}^{N} \exp(x_{i} - m)\right)$

where $m = \max(x_{1}, x_{2}, \dots, x_{N})$.

```{r}
# calculate log posteriors
log_posteriors <- score_values - sapply(X = 1:nrow(score_values), FUN = function(r) {max(score_values[r,]) + log(sum(exp(score_values[r,] - max(score_values[r,]))))})
```

## Score Functions
```{r, fig.keep = "all"}
# plot score function of the first class
plot(data_interval, score_values[,1], type = "l", col = "red", lwd = 2, 
     ylim = c(min(score_values), 0), 
     xlab = "x", ylab = "score", las = 1)
# plot score function of the second class
points(data_interval, score_values[,2], type = "l", col = "green", lwd = 2)
# plot score function of the third class
points(data_interval, score_values[,3], type = "l", col = "blue", lwd = 2)
```

## Posteriors
```{r, fig.keep = "all"}
# plot posterior probability of the first class
plot(data_interval, exp(log_posteriors[,1]), type = "l", col = "red", lwd = 2,
     ylim = c(-0.15, 1), las = 1,
     xlab = "x", ylab = "probability")
# plot posterior probability of the second class
points(data_interval, exp(log_posteriors[,2]), type = "l", col = "green", lwd = 2)
# plot posterior probability of the third class
points(data_interval, exp(log_posteriors[,3]), type = "l", col = "blue", lwd = 2)

class_assignments <- apply(X = score_values, MARGIN = 1, FUN = which.max)
#plot region where the first class has the highest probability
points(data_interval[class_assignments == 1], 
       rep(-0.05, sum(class_assignments == 1)), type = "p", pch = 19, col = "red")
#plot region where the second class has the highest probability
points(data_interval[class_assignments == 2], 
       rep(-0.10, sum(class_assignments == 2)), type = "p", pch = 19, col = "green")
#plot region where the third class has the highest probability
points(data_interval[class_assignments == 3], 
       rep(-0.15, sum(class_assignments == 3)), type = "p", pch = 19, col = "blue")
```
